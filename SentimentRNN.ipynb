{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analisys RNN\n",
    "\n",
    "* [Step 0](#step0): Loading dataset\n",
    "* [Step 1](#step1): Preparing the data\n",
    "* [Step 2](#step2): Building the graph\n",
    "* [Step 3](#step3): Training\n",
    "* [Step 4](#step4): Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "\n",
    "with open('dataset/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  \n",
      "Labels: positive\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "#seeing the first 100 caracters of the reviews file\n",
    "print('Reviews:', reviews[:200])\n",
    "\n",
    "#seeing the first 2 words(17 caracters) of the labels file\n",
    "print('Labels:', labels[:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews:  25001\n",
      "Number of words: 6020196\n"
     ]
    }
   ],
   "source": [
    "#First we have to get rid of the pontuations, so we go \n",
    "#throught all the caracters of the dataset and put them \n",
    "#into a vector only if it is not a punctuation\n",
    "from string import punctuation\n",
    "\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "#as we know, all the reviews are separeted by a \\n so we get the reviews list\n",
    "reviews = all_text.split('\\n')\n",
    "print('number of reviews: ', len(reviews))\n",
    "\n",
    "#And we want also to have an array of words to hash then into a lookup table after\n",
    "all_text = ''.join(reviews)\n",
    "words = all_text.split()\n",
    "print('Number of words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s \n"
     ]
    }
   ],
   "source": [
    "#seeing some reviews without punctuation\n",
    "print(all_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n"
     ]
    }
   ],
   "source": [
    "#Seeing some words\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to create a dictionary to convert words into numbers to pass throught the Neural net\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(words)\n",
    "count_ordered = sorted(count, key=count.get, reverse=True)\n",
    "\n",
    "#creating the hash\n",
    "#we start the hash by 1 because we will padding the comments with zeros after, \n",
    "#so to not consider 0 as a word we start by 1\n",
    "vocab_to_int = {word: i for i,word in enumerate(count_ordered, 1)}\n",
    "\n",
    "#now we can convert all the reviews into integers\n",
    "reviews_int = []\n",
    "for r in reviews:\n",
    "    reviews_int.append([vocab_to_int[word] for word in r.split()])\n",
    "    \n",
    "#And the labels\n",
    "\n",
    "labels = np.array([1 if l == 'positive' else 0 for l in labels.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  [63, 4, 3, 125, 36, 47, 7478, 1398, 16, 3, 4213, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10712, 7450, 300, 6, 667, 83, 35, 2119, 1087, 2998, 34, 1, 901, 58054, 4, 8, 13, 5136, 464, 8, 2662, 1721, 1, 221, 57, 17, 58, 794, 1300, 834, 228, 8, 43, 98, 123, 1470, 59, 147, 38, 1, 965, 142, 29, 667, 123, 1, 13963, 410, 61, 94, 1778, 306, 756, 5, 3, 819, 10404, 22, 3, 1727, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 618, 34, 685, 85, 30016, 31978, 685, 374, 3346, 11622, 2, 16559, 8037, 51, 29, 108, 3336]\n",
      "Labels:  [1 0]\n"
     ]
    }
   ],
   "source": [
    "#Seeing the new reviews and labels\n",
    "r = reviews_int[1]\n",
    "print('Review: ', r[:200])\n",
    "print(\"Labels: \", labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    " from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_int])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the above cell we can see that we have some comments with to fewer words and too long comments\n",
    "#To fix that we will define a length(200). If the comment has less words than that we will pad with zeros otherwise \n",
    "#we will truncate the comment.\n",
    "# First we filter out that review with 0 length\n",
    "non_zero_index = [ii for ii, review in enumerate(reviews_int) if len(review) != 0]\n",
    "\n",
    "reviews_int = [reviews_int[ii] for ii in non_zero_index] \n",
    "labels = np.array([labels[ii] for ii in non_zero_index])\n",
    "\n",
    "#now we create an array of features(initialize of zeros) and get the fist 200 words \n",
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_int), seq_len), dtype=int)\n",
    "for i, rev in enumerate(reviews_int):\n",
    "    features[i, -len(rev):] = np.array(rev)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "#Now we have to create the training, test and validation datasets\n",
    "split_frac = 0.8\n",
    "split_frac_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_frac_idx], features[split_frac_idx:]\n",
    "train_y, val_y = labels[:split_frac_idx], labels[split_frac_idx:]\n",
    "\n",
    "val_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:val_idx], val_x[val_idx:]\n",
    "val_y, test_y = val_y[:val_idx], val_y[val_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Building the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lstm_size = 256  #number of lstm cells\n",
    "lstm_layers = 2  #number of layers\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "embed_size = 300 #number of units in the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cell(num_units, dropout):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=dropout)\n",
    "    \n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #creating the embedding layer\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    #creating the lstm cells\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(lstm_layers)])\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    #to run the data through the network we use tf.nn.dynamic_rnn\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    #once we have the output(only consider the output of the final lstm cell)\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    #validation accuracy\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batching function\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.238\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.238\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.196\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.232\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.213\n",
      "saving model!\n",
      "Val acc: 0.661\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.204\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.193\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.177\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.217\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.217\n",
      "Val acc: 0.652\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.188\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.164\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.311\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.209\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.201\n",
      "saving model!\n",
      "Val acc: 0.667\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.206\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.168\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.167\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.133\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.138\n",
      "saving model!\n",
      "Val acc: 0.733\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.147\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.139\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.103\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.121\n",
      "Epoch: 3/10 Iteration: 125 Train loss: 0.122\n",
      "saving model!\n",
      "Val acc: 0.792\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.123\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.097\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.112\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.141\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.111\n",
      "Val acc: 0.764\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.094\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.091\n",
      "Epoch: 4/10 Iteration: 165 Train loss: 0.103\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.083\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.141\n",
      "Val acc: 0.725\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.136\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.148\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.151\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.171\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.153\n",
      "Val acc: 0.721\n",
      "Epoch: 5/10 Iteration: 205 Train loss: 0.246\n",
      "Epoch: 5/10 Iteration: 210 Train loss: 0.193\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.142\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.118\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.103\n",
      "saving model!\n",
      "Val acc: 0.814\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.130\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.118\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.122\n",
      "Epoch: 6/10 Iteration: 245 Train loss: 0.084\n",
      "Epoch: 6/10 Iteration: 250 Train loss: 0.076\n",
      "Val acc: 0.788\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.077\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.144\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.119\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.125\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.096\n",
      "Val acc: 0.796\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.096\n",
      "Epoch: 7/10 Iteration: 285 Train loss: 0.071\n",
      "Epoch: 7/10 Iteration: 290 Train loss: 0.080\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.069\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.066\n",
      "saving model!\n",
      "Val acc: 0.838\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.061\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.076\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.079\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.076\n",
      "Epoch: 8/10 Iteration: 325 Train loss: 0.069\n",
      "Val acc: 0.803\n",
      "Epoch: 8/10 Iteration: 330 Train loss: 0.082\n",
      "Epoch: 8/10 Iteration: 335 Train loss: 0.062\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.088\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.046\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.047\n",
      "Val acc: 0.826\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.055\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.065\n",
      "Epoch: 9/10 Iteration: 365 Train loss: 0.077\n",
      "Epoch: 9/10 Iteration: 370 Train loss: 0.090\n",
      "Epoch: 9/10 Iteration: 375 Train loss: 0.066\n",
      "Val acc: 0.838\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.081\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.050\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.058\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.161\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.127\n",
      "Val acc: 0.698\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "accuracy_max = 0\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                if(np.mean(val_acc) > accuracy_max):\n",
    "                    print(\"saving model!\")\n",
    "                    accuracy_max = np.mean(val_acc)\n",
    "                    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    #saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
